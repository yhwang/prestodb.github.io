<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Hudi tables via Presto-Hive connector: A Deep Dive · </title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="With the growing popularity of the lakehouse approach, it has become increasingly important for query engines to support these new formats such as Hudi. A [previous blog](https://prestodb.io/blog/2020/08/04/prestodb-and-hudi) discusses the evolution of presto-hudi integration via hive connector at a high level. With the latest community developments, a [separate presto-hudi connector](https://github.com/prestodb/presto/issues/17006) has come up but it is not at par with the hive connector in terms of security features, caching and cost-based optimization. Hive connector has [CachingDirectoryLister](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/CachingDirectoryLister.java) which can be used for caching the splits for a given table for a configurable period of time. It also supports [analyze command](https://prestodb.io/docs/current/sql/analyze.html) which helps in better planning during optimization phase. In this blog, we dive deeper into presto-hudi integration supported via hive connector from a developer&#x27;s perspective and list down the various checkpoints where the query execution moves from presto to hudi library."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Hudi tables via Presto-Hive connector: A Deep Dive · "/><meta property="og:type" content="website"/><meta property="og:url" content="https://prestodb.io/blog/2023/05/30/presto-hudi-deep-dive"/><meta property="og:description" content="With the growing popularity of the lakehouse approach, it has become increasingly important for query engines to support these new formats such as Hudi. A [previous blog](https://prestodb.io/blog/2020/08/04/prestodb-and-hudi) discusses the evolution of presto-hudi integration via hive connector at a high level. With the latest community developments, a [separate presto-hudi connector](https://github.com/prestodb/presto/issues/17006) has come up but it is not at par with the hive connector in terms of security features, caching and cost-based optimization. Hive connector has [CachingDirectoryLister](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/CachingDirectoryLister.java) which can be used for caching the splits for a given table for a configurable period of time. It also supports [analyze command](https://prestodb.io/docs/current/sql/analyze.html) which helps in better planning during optimization phase. In this blog, we dive deeper into presto-hudi integration supported via hive connector from a developer&#x27;s perspective and list down the various checkpoints where the query execution moves from presto to hudi library."/><meta property="og:image" content="https://prestodb.io/img/presto-logo-stacked.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://prestodb.io/img/presto-logo-stacked.png"/><link rel="shortcut icon" href="/img/icon-presto-dots-color.svg"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="stylesheet" href="/css/bootstrap.min.css"/><link rel="stylesheet" href="/css/custom.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://js.hs-scripts.com/39785153.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-presto-white.svg" alt=""/><h2 class="headerTitleWithLogo"></h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/getting-started.html" target="_self">Get Started</a></li><li class=""><a href="/what-is-presto.html" target="_self">Learn</a></li><li class=""><a href="/community.html" target="_self">Community</a></li><li class=""><a href="/blog" target="_self">Blog</a></li><li class=""><a href="/docs/current" target="_blank">Docs</a></li><li class=""><a href="https://communityinviter.com/apps/prestodb/prestodb" target="_blank">Slack</a></li><li class=""><a href="https://github.com/prestodb/presto" target="_blank">GitHub</a></li><li class=""><a href="https://stackoverflow.com/questions/tagged/presto" target="_blank">Stackoverflow</a></li><li class=""><a href="https://twitter.com/prestodb" target="_blank">Twitter</a></li><li class=""><a href="https://www.linkedin.com/company/presto-foundation/" target="_blank">LinkedIn</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Recent Posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Recent Posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2023/07/13/presto-at-bolt">Migrating to Presto - How Bolt Built a Data Platform Architecture for Scalability and Cost Efficiency</a></li><li class="navListItem"><a class="navItem" href="/blog/2023/07/11/announcing-watsonxdata-presto-lakehouse">IBM watsonx.data - a modern open data lakehouse architecture, built on Presto!</a></li><li class="navListItem"><a class="navItem" href="/blog/2023/06/29/presto-at-adobe-prestocon-day">Harnessing Presto - A Deep Dive into Adobe Advertising&#x27;s Three Use Cases</a></li><li class="navListItem"><a class="navItem" href="/blog/2023/06/23/prestocon-day-recap">Recapping PrestoCon Day 2023 - Presto for the Data Lakehouse, Presto at scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2023/06/14/Denodo-joins-presto-foundation">Denodo Joins the Presto Foundation</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2023/05/30/presto-hudi-deep-dive">Hudi tables via Presto-Hive connector: A Deep Dive</a></h1><p class="post-meta">May 30, 2023</p><div class="authorBlock"><p class="post-authorName"><a href="https://www.linkedin.com/in/pratyaksh-sharma-60593769/" target="_blank" rel="noreferrer noopener">Pratyaksh Sharma</a></p></div></header><div><span><p>With the growing popularity of the lakehouse approach, it has become increasingly important for query engines to support these new formats such as Hudi. A <a href="https://prestodb.io/blog/2020/08/04/prestodb-and-hudi">previous blog</a> discusses the evolution of presto-hudi integration via hive connector at a high level. With the latest community developments, a <a href="https://github.com/prestodb/presto/issues/17006">separate presto-hudi connector</a> has come up but it is not at par with the hive connector in terms of security features, caching and cost-based optimization. Hive connector has <a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/CachingDirectoryLister.java">CachingDirectoryLister</a> which can be used for caching the splits for a given table for a configurable period of time. It also supports <a href="https://prestodb.io/docs/current/sql/analyze.html">analyze command</a> which helps in better planning during optimization phase. In this blog, we dive deeper into presto-hudi integration supported via hive connector from a developer's perspective and list down the various checkpoints where the query execution moves from presto to hudi library.</p>
<!--truncate-->
<h2><a class="anchor" aria-hidden="true" id="input-formats-and-record-readers"></a><a href="#input-formats-and-record-readers" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Input formats and Record readers</h2>
<p>Before actually diving into the presto-hudi integration, let us first review a few related concepts. In the Hadoop world, data is stored in a logical file. This file can have various physical layouts (format) on remote storage. A &quot;Path&quot; identifies a file. Further, an &quot;input format&quot; abstraction allows reading portions (splits) of a file. To keep it short and sweet, input formats help generate splits from files which are logical representations of the data. Input formats provide record readers which do the actual reading from these splits.</p>
<p>InputFormat interface exposes <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">2 methods</a> - <code>getSplits()</code> and <code>getRecordReader()</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="creation-of-splits"></a><a href="#creation-of-splits" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Creation of splits</h3>
<p><code>getSplits()</code> method facilitates the creation of splits from actual data files. On a high level, this involves reading the input paths from the configuration object and scanning those paths to get the files present. Files are logically represented as FileStatus. These file paths are then passed to the <code>makeSplit()</code> method which creates the splits for further processing. Below are the snippets of the actual code where these calls are being made.</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/get-splits-call.png" alt="">
<em>Figure 1: <a href="https://github.com/apache/hadoop/blob/86c250a54a586b1db098121c0c052cb3580fe5a4/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L334">listStatus()</a> gets called internally from getSplits().</em></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/make-split-call.png" alt="">
<em>Figure 2: <a href="https://github.com/apache/hadoop/blob/86c250a54a586b1db098121c0c052cb3580fe5a4/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L380">makeSplit()</a> call is highlighted as part of getSplits() implementation.</em></p>
<p>The above snippets are taken from <a href="https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java"><code>FileInputFormat</code></a> class. Please note that the lines are highlighted where these calls are being made.</p>
<h2><a class="anchor" aria-hidden="true" id="hudi-table-typeshttpshudiapacheorgdocstable_types"></a><a href="#hudi-table-typeshttpshudiapacheorgdocstable_types" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="https://hudi.apache.org/docs/table_types/">Hudi table types</a></h2>
<p>Hudi supports two types of tables, namely, Copy on Write (CoW) and Merge on Read (MoR). CoW is the simpler of the two in terms of the reading complexities involved. Upserts made to this table type result in newer versions of columnar parquet files which can be read efficiently using the native <a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetPageSource.java"><code>ParquetPageSource</code></a> used internally by Presto.
MoR involves writing incoming updates to delta log files in avro format by default. These log files are later compacted to create new base parquet files. Since this involves row and columnar format files, this table type supports Read Optimized (RO) and Real Time (RT) queries. RO queries only scan parquet files, while RT queries merge parquet and log files on the fly to generate the latest snapshot.</p>
<p>To be able to query these tables, the table metadata needs to be synced with Hive or Glue metastore. Syncing MoR tables with the configured metastore results in the creation of 2 table types, <code>&lt;table_name&gt;_ro</code> and <code>&lt;table_name&gt;_rt</code>. They are internally differentiated by setting the config <code>hoodie.query.as.ro.table</code>. As the name suggests, the former is used for read optimized queries involving only base parquet files, while the latter is used for real time queries involving the merging of delta log files and base parquet files on the fly.</p>
<p>From Presto’s point of view, CoW table and <code>_ro</code> version of a MoR table behave the same way. However <code>_rt</code> version of MoR table type requires additional work as described in the next section.</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/hudi-mor-ro-1.png" alt=""></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/hudi-mor-ro-2.png" alt="">
<em>Figure 3: Snapshot showing details about _ro table.</em></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/hudi-mor-rt-1.png" alt=""></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/hudi-mor-rt-2.png" alt="">
<em>Figure 4: Snapshot showing details about _rt table.</em></p>
<p>The above figures show the difference between the 2 versions of MoR tables in terms of table properties and input formats involved.</p>
<p>As is clearly visible, Hudi syncs the CoW and <code>_ro</code> tables with <code>HoodieParquetInputFormat</code> while <code>_rt</code> tables are registered with <code>HoodieParquetRealtimeInputFormat</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="merge-on-read-table-type"></a><a href="#merge-on-read-table-type" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Merge on Read table type</h3>
<p>Currently presto supports read optimized and real time queries for MoR tables. After the parser, planner and optimizer phases, splits are generated by Presto which are used for creating presto pages to be rendered back to the client. We focus on this split generation phase and the record reader involved thereafter in the next sections.</p>
<h4><a class="anchor" aria-hidden="true" id="real-time-queries"></a><a href="#real-time-queries" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Real time Queries</h4>
<p>Below custom components are of interest to us for executing real time queries -</p>
<ol>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java">HoodieParquetRealtimeInputFormat</a></li>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimeFileSplit.java">HoodieRealtimeFileSplit</a></li>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/RealtimeFileStatus.java">RealtimeFileStatus</a></li>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimePath.java">HoodieRealtimePath</a></li>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java">RealtimeCompactedRecordReader</a></li>
<li><a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java">HoodieMergedLogRecordScanner</a></li>
</ol>
<p>All the above classes are provided via <code>hudi-hadoop-mr</code> module in hudi repo, and we will see in some time how these classes are triggered.</p>
<p><code>BackgroundHiveSplitLoader</code> performs the task of loading splits and it in turn delegates the task to <code>StoragePartitionLoader.java</code> class. StoragePartitionLoader loads the partitions. After verifying the annotations and inputFormat class <a href="https://github.com/prestodb/presto/blob/20e6b25e90828dd6db49ef89e7750bc9dc75d743/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java#L290">here</a>, <code>inputFormat.getSplits()</code> is called. This is the first place where Hudi’s custom logic is called. Before diving deeper, let us have a look at the class hierarchies below -</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/input-format-hierarchy.png" alt=""></p>
<p><em>Figure 5: Input format hierarchy.</em></p>
<p>HoodieTableInputFormat is an abstract class and is implemented as below -</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/hudi-table-if-impl.png" alt="">
<em>Figure 6: HoodieTableInputFormat implementations.</em></p>
<p>For <code>_rt</code> tables, input format is registered as <code>HoodieParquetRealtimeInputFormat</code>. Hence when <code>inputFormat.getSplits()</code> is called, it calls <code>HoodieParquetInputFormatBase.getSplits()</code> which delegates the call to <code>HoodieTableInputFormat</code> which is implemented as <code>HoodieMergeOnReadTableInputFormat.getSplits()</code>. This makes a call to <code>FileInputFormat.getSplits()</code>.</p>
<p>Now refer to Figure 1, <code>listStatus()</code> is called which is overridden in <code>HoodieCopyOnWriteTableInputFormat.listStatus()</code>. This method in turn calls <code>listStatusForIncrementalMode()</code> and <code>listStatusForSnapshotMode()</code>. This will return <code>RealtimeFileStatus</code>. <code>RealtimeFileStatus</code> overrides <code>getPath()</code> method which returns <code>HoodieRealtimePath</code>. Please refer to the below figures -</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/list-status-mor.png" alt="">
<em>Figure 7: listStatus() call for MoR table type returns RealtimeFileStatus.</em></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/make-split-mor.png" alt=""></p>
<p><em>Figure 8: makeSplit() internally called from getSplits() for MoR table type returns HoodieRealtimeFileSplit.</em></p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/get-path-mor.png" alt=""></p>
<p><em>Figure 9: getPath() is overridden in RealtimeFileStatus and it returns HoodieRealtimePath.</em></p>
<p>Now that real time splits got generated, we need a way of reading the records from these special splits. These splits contain information about the base parquet files as well as delta avro files for the queried snapshot. To be able to generate the snapshot, records from both types of files need to merged on the fly. Native <code>ParquetPageSource</code> we talked about earlier can only scan parquet files, hence we need a custom record reader which is provided by Hudi. <code>GenericHiveRecordCursorProvider</code> calls createRecordReader which ultimately calls <code>inputFormat.getRecordReader()</code>. This returns <code>RealtimeCompactedRecordReader</code> by default. Please refer to the below figure -</p>
<p><img src="/img/blog/2023-05-30-presto-hudi-deep-dive/real-time-record-reader.png" alt="">
<em>Figure 10: Real time record reader.</em></p>
<h4><a class="anchor" aria-hidden="true" id="read-optimized-queries"></a><a href="#read-optimized-queries" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Read Optimized Queries</h4>
<p>These queries will occur from <code>&lt;table_name&gt;_ro</code> table registered in HMS and it has HoodieParquetInputFormat as the input format. These queries will only read the base parquet files, hence the name read optimized. The execution of these queries is exactly similar to that for CoW table type. Please refer to the next section for more details.</p>
<h3><a class="anchor" aria-hidden="true" id="copy-on-write-table-type"></a><a href="#copy-on-write-table-type" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Copy on Write table type</h3>
<p>The layout of this table type only consists of parquet files. This makes use of the <a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/HudiDirectoryLister.java"><code>HudiDirectoryLister</code></a> class for listing relevant data files. <code>HudiDirectoryLister</code> is a singleton object and gets created once every query involving hudi CoW table. This class creates the hudi timeline by scanning the .hoodie folder, then creates a file system view on top of this timeline. <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java"><code>FileSystemView</code></a> is a logical representation of the hudi storage layout which binds the timeline and data files together.</p>
<p>Unlike <a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/HadoopDirectoryLister.java"><code>HadoopDirectoryLister</code></a>, this lister does not make use of the file listing cache since the hudi table is assumed to be ever-evolving and hence the partitions are not sealed. Once the splits are created, native <code>ParquetPageSource</code> is used for rendering the results.</p>
<p>We hope this blog was useful in understanding the code level interaction between presto and hudi. Contributions are welcome to improve this blog and write new ones. Should you have any doubts, please join the Presto <a href="https://communityinviter.com/apps/prestodb/prestodb">slack channel</a> or engage via <a href="https://github.com/prestodb/presto/issues">github issues</a>.</p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent Posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#input-formats-and-record-readers">Input formats and Record readers</a><ul class="toc-headings"><li><a href="#creation-of-splits">Creation of splits</a></li></ul></li><li><a href="#hudi-table-typeshttpshudiapacheorgdocstable_types"><a href="https://hudi.apache.org/docs/table_types/">Hudi table types</a></a><ul class="toc-headings"><li><a href="#merge-on-read-table-type">Merge on Read table type</a></li><li><a href="#copy-on-write-table-type">Copy on Write table type</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="copyright">Copyright © The Presto Foundation.<br/>All rights reserved. Presto is a registered trademark of LF Projects, LLC. <br/>Please see our<a href="https://lfprojects.org/policies/trademark-policy/">Trademark Policy</a> for more information.<br/><a href="https://lfprojects.org/policies/privacy-policy/">Privacy Policy</a> |<a href="https://lfprojects.org/policies/terms-of-use/">Terms of Use</a>.</section></footer></div></body></html>